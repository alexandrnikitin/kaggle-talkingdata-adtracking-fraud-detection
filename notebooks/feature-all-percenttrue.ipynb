{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from dask import bag\n",
    "from dask.diagnostics import ProgressBar\n",
    "from featuretools.primitives import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'ip': 'uint32',\n",
    "    'app': 'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os': 'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8'\n",
    "}\n",
    "to_read = ['app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "to_parse = ['click_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEntitySet(filename):\n",
    "    df = pd.read_csv(filename, usecols=to_read, dtype=dtypes, parse_dates=to_parse)\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "    es = ft.EntitySet(id='clicks')\n",
    "    es = es.entity_from_dataframe(\n",
    "        entity_id='clicks',\n",
    "        dataframe=df,\n",
    "        index='id',\n",
    "        time_index='click_time',\n",
    "        \n",
    "        variable_types={\n",
    "            'app': ft.variable_types.Categorical,\n",
    "            'device': ft.variable_types.Categorical,\n",
    "            'os': ft.variable_types.Categorical,\n",
    "            'channel': ft.variable_types.Categorical,\n",
    "            'is_attributed': ft.variable_types.Boolean,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    es = es.normalize_entity(base_entity_id='clicks', new_entity_id='app', index='app', make_time_index=False)\n",
    "    es = es.normalize_entity(base_entity_id='clicks', new_entity_id='device', index='device', make_time_index=False)\n",
    "    es = es.normalize_entity(base_entity_id='clicks', new_entity_id='os', index='os', make_time_index=False)\n",
    "    es = es.normalize_entity(base_entity_id='clicks', new_entity_id='channel', index='channel', make_time_index=False)\n",
    "    es.add_last_time_indexes()\n",
    "    return es\n",
    "\n",
    "\n",
    "def calc_feature_matrix(es, entity_id, cutoff_time, training_window):\n",
    "    feature_matrix, _ = ft.dfs(\n",
    "        entityset=es,\n",
    "        target_entity=entity_id,\n",
    "        cutoff_time=cutoff_time,\n",
    "        training_window=training_window,\n",
    "        max_depth=3\n",
    "    )\n",
    "\n",
    "    return feature_matrix\n",
    "\n",
    "def createFeatures(filename, entity_sets, target_entity, cutoff_time, training_window):\n",
    "    tw_suffix = training_window.get_name().replace(' ', '').lower()\n",
    "    \n",
    "    b = bag.from_sequence(entity_sets)\n",
    "    feature_matrices = b.map(\n",
    "        calc_feature_matrix, \n",
    "        entity_id=target_entity, \n",
    "        cutoff_time=cutoff_time, \n",
    "        training_window=training_window)\n",
    "    out = feature_matrices.compute()\n",
    "    feature_matrix = pd.concat(out)\n",
    "    feature_matrix.columns = [str(col) + f\"_{target_entity}_{tw_suffix}\" for col in feature_matrix.columns]\n",
    "    \n",
    "    name, ext = os.path.splitext(os.path.basename(filename))\n",
    "    output_file = os.path.dirname(filename) + '/features/' + f\"{name}_{target_entity}_{tw_suffix}_features{ext}\"\n",
    "    feature_matrix.to_csv(output_file)\n",
    "    \n",
    "    del feature_matrix\n",
    "    del feature_matrices\n",
    "    del b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 26.3s\n",
      "Processing: ../data/interim/train_2017-11-08_1000.csv\n",
      "[########################################] | 100% Completed |  2min 55.6s\n"
     ]
    }
   ],
   "source": [
    "# target_entities = ['app', 'device', 'os', 'channel']\n",
    "target_entities = ['channel']\n",
    "filenames_train = sorted(glob('../data/interim/train_2017-11-08_1000.csv'))\n",
    "training_windows = ['1 hours']\n",
    "\n",
    "for target_entity in target_entities:\n",
    "    filenames = glob(f\"../data/interim/partitioned/{target_entity}/train_*.csv\")\n",
    "    b = bag.from_sequence(filenames)\n",
    "    entity_sets = b.map(createEntitySet).compute()\n",
    "    gc.collect()\n",
    "    \n",
    "    for filename in filenames_train:\n",
    "        print(f\"Processing: {filename}\")\n",
    "        df = pd.read_csv(filename, usecols=['click_time'], parse_dates=to_parse)\n",
    "        cutoff_time = df['click_time'].min()\n",
    "        del df\n",
    "        for training_window in training_windows:\n",
    "            createFeatures(filename, entity_sets, target_entity=target_entity, cutoff_time=cutoff_time, training_window=ft.Timedelta(training_window))\n",
    "            gc.collect()\n",
    "    \n",
    "    del entity_sets\n",
    "    del b\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
